# Crawl2Answer

**Crawl. Retrieve. Answer.**

A Q&A support bot using Retrieval Augmented Generation (RAG) that crawls websites, extracts clean textual content, and provides accurate answers based on retrieved information.

## ğŸ¯ Overview

Crawl2Answer is a complete RAG (Retrieval Augmented Generation) system that:

1. **Crawls** websites and extracts clean textual content
2. **Chunks** text into manageable segments
3. **Generates** embeddings for text chunks
4. **Stores** embeddings in a vector database
5. **Retrieves** relevant chunks for user queries
6. **Generates** answers strictly from retrieved content
7. **Exposes** functionality via a REST API

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Web Crawler   â”‚â”€â”€â”€â–¶â”‚  Text Extractor â”‚â”€â”€â”€â–¶â”‚   Text Chunker  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    REST API     â”‚â—€â”€â”€â”€â”‚    Retriever    â”‚â—€â”€â”€â”€â”‚    Embedder     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚                       â”‚
                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                       â”‚  Vector Store   â”‚â—€â”€â”€â”€â”‚  Vector Databaseâ”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“ Project Structure

```
Crawl2Answer/
â”‚
â”œâ”€â”€ crawling/           # Website crawling functionality
â”‚   â””â”€â”€ crawler.py
â”‚
â”œâ”€â”€ extraction/         # HTML to text extraction
â”‚   â””â”€â”€ text_extractor.py
â”‚
â”œâ”€â”€ chunking/           # Text segmentation
â”‚   â””â”€â”€ chunker.py
â”‚
â”œâ”€â”€ embeddings/         # Vector embeddings generation
â”‚   â””â”€â”€ embedder.py
â”‚
â”œâ”€â”€ vector_store/       # Vector database operations
â”‚   â””â”€â”€ vector_db.py
â”‚
â”œâ”€â”€ retrieval/          # Content retrieval and ranking
â”‚   â””â”€â”€ retriever.py
â”‚
â”œâ”€â”€ api/                # REST API endpoints
â”‚   â””â”€â”€ main.py
â”‚
â”œâ”€â”€ config/             # Configuration management
â”‚   â””â”€â”€ settings.py
â”‚
â”œâ”€â”€ data/               # Data storage
â”‚   â”œâ”€â”€ raw/           # Raw crawled content
â”‚   â”œâ”€â”€ processed/     # Cleaned and processed text
â”‚   â””â”€â”€ embeddings/    # Vector database files
â”‚
â”œâ”€â”€ .env               # Environment variables
â”œâ”€â”€ .env.example       # Environment variables template
â”œâ”€â”€ .gitignore         # Git ignore rules
â”œâ”€â”€ requirements.txt   # Python dependencies
â”œâ”€â”€ run.sh            # Unix start script
â”œâ”€â”€ run.bat           # Windows start script
â””â”€â”€ README.md         # This file
```

## ğŸš€ Quick Start

### Prerequisites

- Python 3.8 or higher
- pip (Python package manager)

### Installation & Setup

#### Option 1: Automatic Setup (Recommended)

**Windows:**
```cmd
run.bat
```

**Linux/Mac:**
```bash
chmod +x run.sh
./run.sh
```

#### Option 2: Manual Setup

1. **Clone and navigate to the project**
   ```bash
   cd Crawl2Answer
   ```

2. **Create a virtual environment**
   ```bash
   python -m venv crawl2answer_env
   ```

3. **Activate the virtual environment**
   
   **Windows:**
   ```cmd
   crawl2answer_env\Scripts\activate
   ```
   
   **Linux/Mac:**
   ```bash
   source crawl2answer_env/bin/activate
   ```

4. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   ```

5. **Configure environment**
   ```bash
   cp .env.example .env
   # Edit .env file with your configuration
   ```

6. **Start the API server**
   ```bash
   python -m uvicorn api.main:app --host 0.0.0.0 --port 8000 --reload
   ```

## âš™ï¸ Configuration

Edit the `.env` file to configure the system:

```env
# Website to crawl
BASE_URL=https://docs.python.org/3/

# Embedding model (free option: sentence_transformers)
EMBEDDING_MODEL_TYPE=sentence_transformers
EMBEDDING_MODEL_NAME=all-MiniLM-L6-v2

# For OpenAI embeddings (requires API key)
# EMBEDDING_MODEL_TYPE=openai
# OPENAI_API_KEY=your_api_key_here

# Text processing
CHUNK_SIZE=1000
CHUNK_OVERLAP=200

# API settings
API_PORT=8000
```

## ğŸ§ª Testing

After setting up the environment, you can test the various components:

### Option 1: Test Scripts
```bash
# Test crawler functionality
python tests/test_crawler_simple.py

# Test text extraction  
python tests/test_text_extraction.py

# Test text chunking
python tests/test_chunking_demo.py

# Test full pipeline
python tests/test_full_pipeline.py
```

### Option 2: API Testing
1. Start the API server:
   ```bash
   python -m uvicorn api.main:app --reload
   ```

2. Test individual endpoints:
   ```bash
   # Test crawler
   curl -X POST "http://localhost:8000/test-crawl" \
     -H "Content-Type: application/json" \
     -d '{
       "url": "https://docs.python.org/3/tutorial/",
       "max_pages": 5,
       "max_depth": 2,
       "delay": 1.0
     }'
   
   # Test text extraction
   curl -X POST "http://localhost:8000/test-extraction" \
     -H "Content-Type: application/json" \
     -d '{
       "url": "https://docs.python.org/3/tutorial/introduction.html",
       "delay": 1.0
     }'
   
   # Test text chunking
   curl -X POST "http://localhost:8000/test-chunking" \
     -H "Content-Type: application/json" \
     -d '{
       "url": "https://docs.python.org/3/tutorial/introduction.html",
       "strategy": "smart",
       "delay": 1.0
     }'
```

### Features Tested

#### Crawler Features
- âœ… **Domain Restriction**: Only crawls internal links from the same domain
- âœ… **Smart Filtering**: Skips login pages, PDFs, APIs, and other non-content URLs  
- âœ… **Depth Control**: Limits crawling depth to prevent infinite loops
- âœ… **Rate Limiting**: Configurable delay between requests
- âœ… **Page Data**: Stores URL, title, HTML content, and metadata for each page
- âœ… **Link Extraction**: Finds and follows internal links automatically

#### Text Extraction Features
- âœ… **HTML Parsing**: Robust parsing with BeautifulSoup
- âœ… **Content Cleaning**: Removes navbars, footers, scripts, ads, cookie banners
- âœ… **Smart Detection**: Identifies main content areas automatically
- âœ… **Text Normalization**: Cleans whitespace, removes noise, filters quality
- âœ… **Rich Metadata**: Extracts titles, descriptions, headings, and statistics
- âœ… **Structured Output**: Type-safe data structures with comprehensive information

#### Text Chunking Features
- âœ… **Multiple Strategies**: Smart, fixed, sentence, and paragraph-based chunking
- âœ… **Boundary Detection**: Respects sentence and paragraph boundaries
- âœ… **Configurable Overlap**: Maintains context between chunks
- âœ… **Quality Filtering**: Removes low-quality chunks automatically
- âœ… **Statistical Analysis**: Provides comprehensive chunking metrics
- âœ… **Metadata Preservation**: Maintains source information and context

## ğŸ“– Usage

### 1. Start the API Server

After running the setup script, the API will be available at:
- **API Endpoint:** http://localhost:8000
- **Documentation:** http://localhost:8000/docs
- **Alternative Documentation:** http://localhost:8000/redoc

### 2. Crawl a Website (Enhanced)

```bash
curl -X POST "http://localhost:8000/crawl" \
  -H "Content-Type: application/json" \
  -d '{
    "url": "https://docs.python.org/3/tutorial/",
    "max_pages": 8,
    "max_depth": 2,
    "delay": 1.0
  }'
```

**Response includes:**
- List of crawled URLs with titles
- Domain information and statistics  
- Content size and crawling metadata
- Total pages processed and chunked

### 2a. Test Text Extraction (Single Page)

```bash
curl -X POST "http://localhost:8000/test-extraction" \
  -H "Content-Type: application/json" \
  -d '{
    "url": "https://docs.python.org/3/tutorial/introduction.html",
    "delay": 1.0
  }'
```

**Features demonstrated:**
- HTML parsing and content extraction
- Removal of navigation, ads, and noise
- Main content area detection  
- Text cleaning and normalization
- Rich metadata extraction
- Content quality statistics

### 2b. Test Crawl (No Processing)

```bash
curl -X POST "http://localhost:8000/test-crawl" \
  -H "Content-Type: application/json" \
  -d '{
    "url": "https://docs.python.org/3/tutorial/",
    "max_pages": 5,
    "max_depth": 2
  }'
```

This endpoint only crawls and returns URLs without processing content.

### 3. Ask Questions

```bash
curl -X POST "http://localhost:8000/ask" \
  -H "Content-Type: application/json" \
  -d '{
    "question": "What is a Python list?",
    "max_results": 3
  }'
```

### 4. Check System Status

```bash
curl http://localhost:8000/status
```

## ğŸ”§ API Endpoints

| Method | Endpoint | Description |
|--------|----------|-------------|
| GET | `/` | System information and stats |
| POST | `/crawl` | Crawl website and add to knowledge base (with enhanced text extraction) |
| POST | `/test-crawl` | Test crawl website and return URLs only (no content processing) |
| POST | `/test-extraction` | Test text extraction from a single page (with cleaning preview) |
| POST | `/ask` | Ask a question and get an answer |
| GET | `/status` | Get system status and statistics |
| DELETE | `/clear` | Clear the knowledge base |

## ğŸ›ï¸ Advanced Configuration

### Embedding Models

**Option 1: SentenceTransformers (Free)**
```env
EMBEDDING_MODEL_TYPE=sentence_transformers
EMBEDDING_MODEL_NAME=all-MiniLM-L6-v2
```

**Option 2: OpenAI (Requires API Key)**
```env
EMBEDDING_MODEL_TYPE=openai
EMBEDDING_MODEL_NAME=text-embedding-ada-002
OPENAI_API_KEY=your_api_key_here
```

### Text Chunking

Adjust chunk size and overlap for optimal performance:
```env
CHUNK_SIZE=1000        # Characters per chunk
CHUNK_OVERLAP=200      # Overlap between chunks
```

### Retrieval Settings

Configure retrieval behavior:
```env
RETRIEVAL_K=5              # Number of chunks to retrieve
SIMILARITY_THRESHOLD=0.1   # Minimum similarity score
```

## ğŸ§ª Example Workflow

1. **Configure your target website** in `.env`
2. **Start the API server** using `run.bat` or `run.sh`
3. **Crawl the website:**
   ```json
   POST /crawl
   {
     "url": "https://docs.python.org/3/",
     "max_pages": 10
   }
   ```
4. **Ask questions:**
   ```json
   POST /ask
   {
     "question": "How do I create a Python function?",
     "max_results": 5
   }
   ```
5. **Get structured answers** with source references

## ğŸ› ï¸ Development

### Project Components

- **Crawler:** Fetches web pages with rate limiting
- **Text Extractor:** Cleans HTML and extracts readable content
- **Chunker:** Splits text into overlapping segments
- **Embedder:** Generates vector representations using SentenceTransformers or OpenAI
- **Vector Store:** FAISS-based similarity search
- **Retriever:** Finds relevant content for queries
- **API:** FastAPI-based REST interface

### Adding New Features

1. Each component is modular and can be extended independently
2. Add new endpoints in `api/main.py`
3. Extend configuration in `config/settings.py`
4. Update requirements in `requirements.txt`

## ğŸ› Troubleshooting

**Common Issues:**

1. **Import errors:** Make sure virtual environment is activated
2. **Port conflicts:** Change `API_PORT` in `.env`
3. **Memory issues:** Reduce `CHUNK_SIZE` or `MAX_PAGES`
4. **OpenAI errors:** Check your API key in `.env`

**Logs:**
- Check console output for detailed error messages
- Adjust `LOG_LEVEL` in `.env` for more/less verbose logging

## ğŸ“‹ Requirements

- Python 3.8+
- 4GB+ RAM (for embedding models)
- Internet connection (for crawling and downloading models)
- ~500MB disk space (for models and data)

## ğŸ—ï¸ Built With

- **FastAPI** - REST API framework
- **SentenceTransformers** - Embedding generation
- **FAISS** - Vector similarity search
- **BeautifulSoup** - HTML parsing
- **Requests** - HTTP client
- **Pydantic** - Data validation

## ğŸ“„ License

This project is open source and available under the MIT License.

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Submit a pull request

## ğŸ“ Support

For questions and support:
- Check the API documentation at `/docs`
- Review the configuration in `.env`
- Check the console logs for detailed error messages
